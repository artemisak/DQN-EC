# -*- coding: utf-8 -*-
"""Visualization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13ek-H0rcJStZyfYpokxCPE72iJUDTngL

## 1. Install Dependencies
"""

!pip install pettingzoo[mpe] torch imageio pillow numpy -q

"""## 2. Clone Source Repository"""

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/artemisak/DQN-EC.git
# %cd DQN-EC/toy_example

import numpy as np
import torch
from train_toy_models import DoubleDQNAgent, SEED
from pettingzoo.mpe import simple_speaker_listener_v4
from PIL import Image, ImageDraw, ImageFont
import imageio
from IPython.display import Image as IPImage, display
import pandas as pd

"""## 3. Create GIF Visualization

"""

def load_and_visualize_comparison(
    model_indices=[0, 1],
    output_path="comparison_visualization.gif",
    num_episodes=3,
    max_steps=25,
    fps=10
):

    results = []
    all_frames = {idx: [] for idx in model_indices}

    for model_idx in model_indices:

        env = simple_speaker_listener_v4.parallel_env(
            max_cycles=max_steps,
            continuous_actions=False,
            render_mode="rgb_array"
        )

        agents = {}
        for agent_name in env.possible_agents:
            obs_space = env.observation_space(agent_name)
            act_space = env.action_space(agent_name)
            agents[agent_name] = DoubleDQNAgent(obs_space.shape[0], act_space.n)

            # Load weights with model index
            model_path = f"models/{agent_name.split('_')[0]}_{model_idx}_ddqn.pth"
            agents[agent_name].q_network.load_state_dict(
                torch.load(model_path, weights_only=True, map_location=torch.device('cpu'))
            )
            agents[agent_name].epsilon = 0.0

        for episode in range(num_episodes):
            observations, infos = env.reset(seed=SEED + episode + 3)
            episode_reward = {agent: 0 for agent in env.possible_agents}

            for step in range(max_steps):
                frame = env.render()
                if frame is not None:
                    all_frames[model_idx].append(frame)

                actions = {}
                for agent_name in env.agents:
                    actions[agent_name] = agents[agent_name].select_action(
                        observations[agent_name],
                        training=False
                    )

                next_observations, rewards, terminations, truncations, infos = env.step(actions)

                for agent_name in env.agents:
                    episode_reward[agent_name] += rewards[agent_name]

                observations = next_observations

                if not env.agents:
                    break

            total_reward = sum(episode_reward.values())
            speaker_reward = episode_reward['speaker_0']
            listener_reward = episode_reward['listener_0']

            results.append({
                'Model': model_idx,
                'Episode': episode + 1,
                'Total Reward': round(total_reward, 2),
                'Speaker Reward': round(speaker_reward, 2),
                'Listener Reward': round(listener_reward, 2)
            })

        env.close()

    # Create side-by-side comparison frames
    combined_frames = []
    max_frames = max(len(all_frames[idx]) for idx in model_indices)

    for frame_idx in range(max_frames):
        frames_to_combine = []

        for model_idx in model_indices:
            if frame_idx < len(all_frames[model_idx]):
                frame = all_frames[model_idx][frame_idx]
            else:
                # Use last frame if this model has fewer frames
                frame = all_frames[model_idx][-1]

            # Convert to PIL Image and add label
            img = Image.fromarray(frame)

            # Resize to 50% of original size
            scale_factor = 0.5
            new_width = int(img.width * scale_factor)
            new_height = int(img.height * scale_factor)
            img = img.resize((new_width, new_height), Image.Resampling.LANCZOS)

            draw = ImageDraw.Draw(img)

            # Add model label
            try:
                font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", 24)
            except:
                font = ImageFont.load_default()

            label = f"Model {model_idx}"
            draw.text((10, 10), label, fill=(255, 255, 255), font=font)

            frames_to_combine.append(np.array(img))

        # Combine frames horizontally
        combined = np.hstack(frames_to_combine)
        combined_frames.append(combined)

    if combined_frames:
        imageio.mimsave(output_path, combined_frames, fps=fps)
    else:
        print("No frames captured!")

    # Create comparison table
    df = pd.DataFrame(results)

    return output_path, df

"""## 4. Display"""

# Run the comparison
gif_path, results_df = load_and_visualize_comparison(
    model_indices=[0, 1],
    output_path="comparison_visualization.gif",
    num_episodes=1,
    max_steps=25,
    fps=10
)

# Display the GIF
print("\n" + "="*60)
print("Side-by-Side Comparison Visualization")
print("="*60 + "\n")
display(IPImage(filename=gif_path))